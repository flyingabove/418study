---
title: "MLMLNotes"
author: "Christian Gao"
date: "6/11/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Machine learning

**1.** Describe briefly the steps in a typical data science project.

DataCleaning Train Validation Test 

**2.** Describe the supervised learning problem in general.

You have the answers train a model to recognize features

**3.** Why do we need a test set? 

To prevent overfitting

**4.** Why are evaluation metrics calculated from the training set misleading?

Biased Towards Overfitting

**5.** What is gradient descent?

a^(n+1) = a^n - gdF(a^n)
Stepwise process for moving towards a minimum.

**6.** How do we do cross validation for model evaluation?

Leave-one-out cross-validation (Exhaustive)
Error added from left out obs.

k-fold cross-validation
Dataset segmented then choose 1 out of N segments to be test

**7.** How does the train and test error change vs model complexity?



**8.** Describe overfitting.

**9.** How do we select models with grid/random search?

**10.** What is an ROC curve?

**11.** What is regularization? Give an example.

**12.** Describe the LASSO.

**13.** Describe decision trees and how we fit them.

**14.** Compare random forests and GBMs.

**15.** Describe the main tuning parameters for GBMs. How do they relate to model complexity?

**16.** Describe 3 tricks in training neural networks. 

**17.** Describe grid and random search for hyper-parameter tuning.

**18.** What are ensembles? What are some of the advantages and disadvantages of using them.

**19.** Describe k-means clustering.

**20.** Describe hierarchical clustering.


